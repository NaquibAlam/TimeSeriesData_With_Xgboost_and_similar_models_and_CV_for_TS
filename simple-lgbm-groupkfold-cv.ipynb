{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "In my baseline validation set consist on the last 28 days of the training data. \n",
    "\n",
    "The problem in doing this is the following:\n",
    "\n",
    "Are we sure that this validation aligns with the test set?? (unknown demand and behaviour)\n",
    "\n",
    "We really don't know, thats what we need to predict xD.\n",
    "\n",
    "For this reason the best solution is to make a model that can generalize to unseen data (unknown demand and behaviour). Here we are going to validate the entire training data with GroupKFold strategy to avoid data leakage.\n",
    "\n",
    "The main point on doing this is that the mean of different validations can generalize to a wide range of cases, we will not have the best model for our test but we reduce the chance of overfitting and have a horrible score.\n",
    "\n",
    "Best model is to get a validation that is really similar to the test set, but we don't know which one it is, this is unknown, we can make some guesses or found a statistical ground to choose the right validation set.\n",
    "\n",
    "Another problem that we face is the loss function. Root mean squared error is not align with our competition metric (lower rmse does not guarantee a smaller wrmsse). For this we will use an asymmetric mean squared error loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from typing import Union\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/m5-forecasting-kaggle/Code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read our data\n",
    "def read_data():\n",
    "    # data_small.pkl has been prepared using the all the raw competition data and data_small.pkl is provided as input data for this notebook.\n",
    "    # read data\n",
    "#     data = pd.read_pickle('/kaggle/input/m5-reduce-data/data_small.pkl')\n",
    "    data = pd.read_pickle('../Data/data_small.pkl')\n",
    "    # fillna and label encode categorical features\n",
    "    data = transform(data)\n",
    "    # read submission\n",
    "    submission = pd.read_csv('../Data/raw/sample_submission.csv')\n",
    "    return data, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filla na and label encode categorical features\n",
    "def transform(data):\n",
    "    \n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    #fill all NaNs in event related columns with unknown and this unknown will be treated as a level \n",
    "    #when these columns are provided as cat vars in lightgbm.\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "        \n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', \n",
    "           'event_name_2', 'event_type_2']\n",
    "    # We are label encoding the categorical variables. These label enocded variables will go as categorical varibales in lightgbm.\n",
    "    # We can also use other categorical encoding such Target, Kfold target, LOO target, Catboost encoding etc. which might improve results.\n",
    "    # We are using label encoding for the sake of simplicity\n",
    "    for feature in cat:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "        \n",
    "    # reduce memory usage\n",
    "    data = reduce_mem_usage(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are some points which are worth taking note of w.r.t feature engineering and validation strategy:\n",
    "\n",
    "* Actually here for validation I am using __GroupKFold CV__ and it does have future data in it. But it all depends on how you build your features. If i used lag(t-1) or a few other small lags, the leakage would be huge but I am only using rolling features (there would be leakage but itÂ´s hard for model to find a leak when I am using __GroupKFold CV__ with no lag features).\n",
    "\n",
    "* Leakage in time series is introduced if future data (usually test data) is used for training and then predicting on history (usually train data). When we do normal KFold CV there is a huge chance of leakage since randomization/shuffling will cause training on future and then predicting on history. But here we are using GroupKFold CV (year, and week group), which will make leakage less prominent to a certain extent.\n",
    "\n",
    "* Leakage depends on the features you are using + cv validation strategy. If you check i made a groupkfold strategy for each week, year. In other words there will be some week missing in training.\n",
    "\n",
    "* For example if you use lag(t-1), there is a really big chance of leakage. But if you check my features there are all rolling means, std for _demand_ variable.\n",
    "\n",
    "* On the other hand you can validate with future, past or in between data. Most time series guide recommend __future validation__ or __time series split validation__ or __Walk forward validation__ but in my experience and other competition groupkfold using time batches work as well. Im not saying my model does not have leakeage, but it has somewhat less leakage because of features I am using and validation strategy (__GroupKFold CV__).\n",
    "\n",
    "* The advantage of doing GroupKfold against time series split is that you validate the entire training data (just like vanilla KFold) but downside is there is still some chance of leakage since we might be training on future data and predicting on history while training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple feature ingineer function\n",
    "def simple_fe(data):\n",
    "    \n",
    "    DAYS_PRED = 28 #We have to forecast for 28 days out. that's why last 28 days of data has to be removed/shifted out.\n",
    "    periods = [7, 15, 30, 90] #rolling mean/std windows\n",
    "\n",
    "    # Demand features\n",
    "\n",
    "    ## Lag features. These lag features will introduce huge leakage with GroupKFold CV. That's why we are avoiding it.\n",
    "    \n",
    "    # for diff in [0, 1, 2]:\n",
    "    #         shift = DAYS_PRED + diff\n",
    "    #         data[f\"shift_t{shift}\"] = data.groupby('id')['demand'].transform(lambda x: x.shift(shift))\n",
    "\n",
    "    ## Rolling mean features\n",
    "    ## before taking the rolling mean/std we have to shift by DAYS_PRED so that we have these features available for test data too.\n",
    "    for period in periods:\n",
    "        data['demand_rolling_mean_t' + str(period)] = data.groupby('id')['demand'].transform(lambda x: x.shift(DAYS_PRED).rolling(period).mean())\n",
    "        data['demand_rolling_std_t' + str(period)] = data.groupby('id')['demand'].transform(lambda x: x.shift(DAYS_PRED).rolling(period).std())\n",
    "\n",
    "    # Price features\n",
    "\n",
    "    data[\"shift_price_t1\"] = data.groupby([\"id\"])[\"sell_price\"].transform(lambda x: x.shift(1))\n",
    "    data[\"price_change_t1\"] = (data[\"shift_price_t1\"] - data[\"sell_price\"]) / (data[\"shift_price_t1\"]) #rate of price change day by day\n",
    "    data[\"rolling_price_max_t365\"] = data.groupby('id')[\"sell_price\"].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "    data[\"price_change_t365\"] = (data[\"rolling_price_max_t365\"] - data[\"sell_price\"]) / (data[\"rolling_price_max_t365\"]) #rate of price change over a year\n",
    "    data[\"rolling_price_std_t7\"] = data.groupby('id')[\"sell_price\"].transform(lambda x: x.rolling(7).std())\n",
    "    data[\"rolling_price_std_t30\"] = data.groupby('id')[\"sell_price\"].transform(lambda x: x.rolling(30).std())\n",
    "\n",
    "\n",
    "    # reduce memory\n",
    "    data = reduce_mem_usage(data)\n",
    "    # get time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    time_features = [\"year\", \"quarter\", \"month\", \"week\", \"day\",\"dayofweek\", 'dayofyear', \"is_year_end\", \"is_year_start\", \n",
    "                     \"is_quarter_end\", \"is_quarter_start\", \"is_month_end\", \"is_month_start\"]\n",
    "#     time_features = [\"year\", \"quarter\", \"month\", \"week\", \"day\",\"dayofweek\", 'dayofyear']\n",
    "    dtype = np.int16\n",
    "    for time_feature in time_features:\n",
    "        data[time_feature] = getattr(data['date'].dt, time_feature).astype(dtype)\n",
    "    \n",
    "    data[\"is_weekend\"] = data[\"dayofweek\"].isin([5, 6]).astype(dtype)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined/custom objective function. This is how custom objective/loss function is used in LighGBM and similar models like XgBoost and Catboost\n",
    "# f(y_preds: array, train_data: Dataset) -> grad: array, hess: array  -- With Lightgbm API\n",
    "# f(y_preds: array, y_true: array) -> grad: array, hess: array  -- With Sklearn API\n",
    "# read more about different custom loss function for Xgboost/lightgbm here https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
    "\n",
    "def custom_asymmetric_train(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n",
    "    hess = np.where(residual < 0, 2, 2 * 1.15)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined/custom eval metric\n",
    "# f(y_preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool -- With Lightgbm API\n",
    "# f(y_preds: array, y_true: array) -> name: string, eval_result: float, is_higher_better: bool -- With Sklearn API\n",
    "\n",
    "# NOTE: when you do customized loss function, the default prediction value is margin\n",
    "# This may make built-in evalution metric calculate wrong results\n",
    "# For example, we are doing log likelihood loss, the prediction is score before logistic transformation\n",
    "# Keep this in mind when you use the customization\n",
    "\n",
    "def custom_asymmetric_valid(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    loss = np.where(residual < 0, (residual ** 2) , (residual ** 2) * 1.15) \n",
    "    return \"custom_asymmetric_eval\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lgbm simple model\n",
    "def run_lgb(data, features, cat_features):\n",
    "    \n",
    "    # reset_index\n",
    "    data.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    # going to evaluate with the last 28 days\n",
    "    x_train = data[data['date'] <= '2016-04-24'] #end date in the data is 2016-05-22. Thats's why leaving last 28 days (before '2016-04-25') is train.\n",
    "    y_train = x_train['demand']\n",
    "    test = data[data['date'] >= '2016-04-25'] #end date in the data is 2016-05-22. Thats's why last 28 days (starting from  '2016-04-25') is test.\n",
    "\n",
    "    # define random hyperparammeters\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,\n",
    "        'learning_rate': 0.1,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 1, \n",
    "        'colsample_bytree': 0.85,\n",
    "        'colsample_bynode': 0.85,\n",
    "        'min_data_per_leaf': 25,\n",
    "        'num_leaves': 200,\n",
    "        'lambda_l1': 0.5,\n",
    "        'lambda_l2': 0.5}\n",
    "    \n",
    "    oof = np.zeros(len(x_train))\n",
    "    preds = np.zeros(len(test))\n",
    "    \n",
    "    # GroupKFold by week, month to avoid leakage and overfitting (not entirely sure xD)\n",
    "    kf = GroupKFold(5)\n",
    "    # get subgroups for each week, year pair\n",
    "    group = x_train['week'].astype(str) + '_' + x_train['year'].astype(str)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(x_train, y_train, group)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        train_set = lgb.Dataset(x_train.iloc[trn_idx][features], y_train.iloc[trn_idx], \n",
    "                                categorical_feature = cat_features)\n",
    "        val_set = lgb.Dataset(x_train.iloc[val_idx][features], y_train.iloc[val_idx], \n",
    "                              categorical_feature = cat_features)\n",
    "        \n",
    "        # train with our custom loss function and evaluation metric \n",
    "        model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 50, \n",
    "                          valid_sets = [train_set, val_set], verbose_eval = 50, fobj = custom_asymmetric_train, \n",
    "                          feval = custom_asymmetric_valid)\n",
    "    \n",
    "        # predict oof\n",
    "        oof[val_idx] = model.predict(x_train.iloc[val_idx][features])\n",
    "\n",
    "        # predict test\n",
    "        preds += model.predict(test[features]) / 5\n",
    "        \n",
    "        print('-'*50)\n",
    "        print('\\n')\n",
    "        \n",
    "    oof_rmse = np.sqrt(metrics.mean_squared_error(y_train, oof))\n",
    "    print(f'Our out of folds rmse is {oof_rmse}')\n",
    "        \n",
    "    test = test[['id', 'date', 'demand']]\n",
    "    test['demand'] = preds\n",
    "    return test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the predictions in the correct format\n",
    "def predict(test, submission):\n",
    "    predictions = pd.pivot(test, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "    evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "    validation = submission[['id']].merge(predictions, on = 'id')\n",
    "    final = pd.concat([validation, evaluation])\n",
    "    final.to_csv('submission_custom_loss.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30490, 1947), (30490, 1919), (60980, 29), (1969, 14), (6841121, 4))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval= pd.read_csv(\"../Data/raw/sales_train_evaluation.csv\")\n",
    "train_valid= pd.read_csv(\"../Data/raw/sales_train_validation.csv\")\n",
    "submission= pd.read_csv(\"../Data/raw/sample_submission.csv\")\n",
    "calendar= pd.read_csv(\"../Data/raw/calendar.csv\")\n",
    "sell_price= pd.read_csv(\"../Data/raw/sell_prices.csv\")\n",
    "train_eval.shape, train_valid.shape, submission.shape, calendar.shape, sell_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_eval.head())\n",
    "# print(train_valid.head())\n",
    "# print(calendar.head())\n",
    "# print(sell_price.head())\n",
    "# print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading our data...\n",
      "Mem. usage decreased to 1540.89 Mb (57.1% reduction)\n",
      "(31681090, 18)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "print('Reading our data...')\n",
    "data, submission = read_data()\n",
    "print(data.shape)\n",
    "# print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1011 days of training history\n",
      "we have 163 days left\n",
      "We have enought training data, lets continue\n"
     ]
    }
   ],
   "source": [
    "#Check if we have enough data available for training\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "# get amount of unique days in our data\n",
    "days = abs((data['date'].min() - data['date'].max()).days)\n",
    "# how many training data do we need to train with at least 2 years and consider lags\n",
    "need = 365 + 365 + 90 + 28 #90 +28 because we are taking max rolling mean of 90 days and last 28 days are for test data.\n",
    "print(f'We have {(days - 28)} days of training history')\n",
    "print(f'we have {(days - 28 - need)} days left')\n",
    "if (days - 28 - need) > 0:\n",
    "    print('We have enought training data, lets continue')\n",
    "else:\n",
    "    print('Get more training data, training can fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simple feature engineering...\n",
      "Mem. usage decreased to 2386.86 Mb (37.8% reduction)\n",
      "(31681090, 46)\n"
     ]
    }
   ],
   "source": [
    "# simple feature engineer\n",
    "print('Running simple feature engineering...')\n",
    "data = simple_fe(data)\n",
    "print(data.shape)\n",
    "# print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the count and percentage of NAs right after feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dept_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>state_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>demand</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>part</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>date</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wm_yr_wk</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>event_name_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>event_type_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>event_name_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>event_type_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>snap_CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>snap_TX</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sell_price</td>\n",
       "      <td>1783752</td>\n",
       "      <td>5.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>demand_rolling_mean_t7</td>\n",
       "      <td>1036660</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>demand_rolling_std_t7</td>\n",
       "      <td>1036660</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>demand_rolling_mean_t15</td>\n",
       "      <td>1280580</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>demand_rolling_std_t15</td>\n",
       "      <td>1280580</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>demand_rolling_mean_t30</td>\n",
       "      <td>1737930</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>demand_rolling_std_t30</td>\n",
       "      <td>1737930</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>demand_rolling_mean_t90</td>\n",
       "      <td>3567330</td>\n",
       "      <td>11.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>demand_rolling_std_t90</td>\n",
       "      <td>3567330</td>\n",
       "      <td>11.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>shift_price_t1</td>\n",
       "      <td>1814242</td>\n",
       "      <td>5.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>price_change_t1</td>\n",
       "      <td>1814242</td>\n",
       "      <td>5.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rolling_price_max_t365</td>\n",
       "      <td>12891553</td>\n",
       "      <td>40.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>price_change_t365</td>\n",
       "      <td>12891553</td>\n",
       "      <td>40.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rolling_price_std_t7</td>\n",
       "      <td>1966692</td>\n",
       "      <td>6.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rolling_price_std_t30</td>\n",
       "      <td>2667962</td>\n",
       "      <td>8.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>year</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>quarter</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>month</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>week</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>day</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dayofweek</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>dayofyear</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>is_year_end</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>is_year_start</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>is_quarter_end</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>is_quarter_start</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>is_month_end</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>is_month_start</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>is_weekend</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      index         0      0\n",
       "0                        id         0   0.00\n",
       "1                   item_id         0   0.00\n",
       "2                   dept_id         0   0.00\n",
       "3                    cat_id         0   0.00\n",
       "4                  store_id         0   0.00\n",
       "5                  state_id         0   0.00\n",
       "6                    demand         0   0.00\n",
       "7                      part         0   0.00\n",
       "8                      date         0   0.00\n",
       "9                  wm_yr_wk         0   0.00\n",
       "10             event_name_1         0   0.00\n",
       "11             event_type_1         0   0.00\n",
       "12             event_name_2         0   0.00\n",
       "13             event_type_2         0   0.00\n",
       "14                  snap_CA         0   0.00\n",
       "15                  snap_TX         0   0.00\n",
       "16                  snap_WI         0   0.00\n",
       "17               sell_price   1783752   5.63\n",
       "18   demand_rolling_mean_t7   1036660   3.27\n",
       "19    demand_rolling_std_t7   1036660   3.27\n",
       "20  demand_rolling_mean_t15   1280580   4.04\n",
       "21   demand_rolling_std_t15   1280580   4.04\n",
       "22  demand_rolling_mean_t30   1737930   5.49\n",
       "23   demand_rolling_std_t30   1737930   5.49\n",
       "24  demand_rolling_mean_t90   3567330  11.26\n",
       "25   demand_rolling_std_t90   3567330  11.26\n",
       "26           shift_price_t1   1814242   5.73\n",
       "27          price_change_t1   1814242   5.73\n",
       "28   rolling_price_max_t365  12891553  40.69\n",
       "29        price_change_t365  12891553  40.69\n",
       "30     rolling_price_std_t7   1966692   6.21\n",
       "31    rolling_price_std_t30   2667962   8.42\n",
       "32                     year         0   0.00\n",
       "33                  quarter         0   0.00\n",
       "34                    month         0   0.00\n",
       "35                     week         0   0.00\n",
       "36                      day         0   0.00\n",
       "37                dayofweek         0   0.00\n",
       "38                dayofyear         0   0.00\n",
       "39              is_year_end         0   0.00\n",
       "40            is_year_start         0   0.00\n",
       "41           is_quarter_end         0   0.00\n",
       "42         is_quarter_start         0   0.00\n",
       "43             is_month_end         0   0.00\n",
       "44           is_month_start         0   0.00\n",
       "45               is_weekend         0   0.00"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_na = pd.concat([pd.DataFrame(data.isnull().sum().reset_index()), pd.DataFrame(np.round((data.isnull().sum().values/data.shape[0])*100, 2))], axis=1)\n",
    "count_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique IDs:  30490\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique IDs: \",len(data['id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observation regarding couont of NAs post feature egnineering in above cell are as follows:\n",
    "* We have total _30490_ unique IDs in the data.\n",
    "* When we create rolling features for demand variable which doesn't have any NA, we end up having $ 28+ (rolling\\_window -1) $ NAs for each of the IDs for each of these rolling features. Since we have 30490 unique IDs, total we will have $ 30490 * (28+ (rolling\\_window -1)) $ NAs, where rolling window is an element from $[7, 15, 30, 90]$.\n",
    "* When we have NAs is _demand_ variable too, then the number of NAs in rolling features of demand will have different count on NAs.\n",
    "* To get rid of these NAs, we remove first 118 days of data for each of the IDs in the following cell. After this operation we have zero NAs for these rolling _demand_ features.\n",
    "* With the same logic we can also find the count of NAs for engineered features from _sell_price_ variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing first 118 days\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dept_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>state_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>demand</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>part</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>date</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wm_yr_wk</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>event_name_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>event_type_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>event_name_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>event_type_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>snap_CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>snap_TX</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>snap_WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sell_price</td>\n",
       "      <td>1165543</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>demand_rolling_mean_t7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>demand_rolling_std_t7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>demand_rolling_mean_t15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>demand_rolling_std_t15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>demand_rolling_mean_t30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>demand_rolling_std_t30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>demand_rolling_mean_t90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>demand_rolling_std_t90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>shift_price_t1</td>\n",
       "      <td>1170377</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>price_change_t1</td>\n",
       "      <td>1170377</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rolling_price_max_t365</td>\n",
       "      <td>9291753</td>\n",
       "      <td>33.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>price_change_t365</td>\n",
       "      <td>9291753</td>\n",
       "      <td>33.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rolling_price_std_t7</td>\n",
       "      <td>1194569</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rolling_price_std_t30</td>\n",
       "      <td>1307254</td>\n",
       "      <td>4.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>year</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>quarter</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>month</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>week</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>day</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dayofweek</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>dayofyear</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>is_year_end</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>is_year_start</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>is_quarter_end</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>is_quarter_start</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>is_month_end</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>is_month_start</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>is_weekend</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      index        0      0\n",
       "0                        id        0   0.00\n",
       "1                   item_id        0   0.00\n",
       "2                   dept_id        0   0.00\n",
       "3                    cat_id        0   0.00\n",
       "4                  store_id        0   0.00\n",
       "5                  state_id        0   0.00\n",
       "6                    demand        0   0.00\n",
       "7                      part        0   0.00\n",
       "8                      date        0   0.00\n",
       "9                  wm_yr_wk        0   0.00\n",
       "10             event_name_1        0   0.00\n",
       "11             event_type_1        0   0.00\n",
       "12             event_name_2        0   0.00\n",
       "13             event_type_2        0   0.00\n",
       "14                  snap_CA        0   0.00\n",
       "15                  snap_TX        0   0.00\n",
       "16                  snap_WI        0   0.00\n",
       "17               sell_price  1165543   4.15\n",
       "18   demand_rolling_mean_t7        0   0.00\n",
       "19    demand_rolling_std_t7        0   0.00\n",
       "20  demand_rolling_mean_t15        0   0.00\n",
       "21   demand_rolling_std_t15        0   0.00\n",
       "22  demand_rolling_mean_t30        0   0.00\n",
       "23   demand_rolling_std_t30        0   0.00\n",
       "24  demand_rolling_mean_t90        0   0.00\n",
       "25   demand_rolling_std_t90        0   0.00\n",
       "26           shift_price_t1  1170377   4.17\n",
       "27          price_change_t1  1170377   4.17\n",
       "28   rolling_price_max_t365  9291753  33.09\n",
       "29        price_change_t365  9291753  33.09\n",
       "30     rolling_price_std_t7  1194569   4.25\n",
       "31    rolling_price_std_t30  1307254   4.66\n",
       "32                     year        0   0.00\n",
       "33                  quarter        0   0.00\n",
       "34                    month        0   0.00\n",
       "35                     week        0   0.00\n",
       "36                      day        0   0.00\n",
       "37                dayofweek        0   0.00\n",
       "38                dayofyear        0   0.00\n",
       "39              is_year_end        0   0.00\n",
       "40            is_year_start        0   0.00\n",
       "41           is_quarter_end        0   0.00\n",
       "42         is_quarter_start        0   0.00\n",
       "43             is_month_end        0   0.00\n",
       "44           is_month_start        0   0.00\n",
       "45               is_weekend        0   0.00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Removing first 118 days')\n",
    "# eliminate the first 118 days of our train data because of lags/rolling mean (max 90 days) and 28 days for test data.\n",
    "min_date = data['date'].min() + timedelta(days = 118) #\n",
    "data = data[data['date'] > min_date]\n",
    "\n",
    "# NA count after dropping initial 118 days of data\n",
    "count_na = pd.concat([pd.DataFrame(data.isnull().sum().reset_index()), pd.DataFrame(np.round((data.isnull().sum().values/data.shape[0])*100, 2))], axis=1)\n",
    "count_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date:  2013-11-14 00:00:00\n",
      "End date: 2016-05-22 00:00:00\n"
     ]
    }
   ],
   "source": [
    "min_date= data[\"date\"].min()\n",
    "max_date= data[\"date\"].max()\n",
    "print(\"Start date: \", min_date)\n",
    "print(\"End date:\", max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the number of levels in each of the categorical variables which will go into lightgbm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id 3049\n",
      "dept_id 7\n",
      "cat_id 3\n",
      "store_id 10\n",
      "state_id 3\n",
      "event_name_1 31\n",
      "event_type_1 5\n",
      "event_name_2 3\n",
      "event_type_2 3\n"
     ]
    }
   ],
   "source": [
    "cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', \n",
    "           'event_name_2', 'event_type_2']\n",
    "for col in cat:\n",
    "    print(col, len(data.loc[:,col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model with 38 features...\n",
      "Training fold 1\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.514820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6589\n",
      "[LightGBM] [Info] Number of data points in the train set: 21800350, number of used features: 38\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 5.05764\tvalid_1's custom_asymmetric_eval: 5.66431\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.66307\tvalid_1's custom_asymmetric_eval: 5.48894\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.46178\tvalid_1's custom_asymmetric_eval: 5.42989\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.31945\tvalid_1's custom_asymmetric_eval: 5.39187\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.19642\tvalid_1's custom_asymmetric_eval: 5.35671\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.1014\tvalid_1's custom_asymmetric_eval: 5.33391\n",
      "[350]\ttraining's custom_asymmetric_eval: 4.01658\tvalid_1's custom_asymmetric_eval: 5.31915\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.94338\tvalid_1's custom_asymmetric_eval: 5.30522\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.87916\tvalid_1's custom_asymmetric_eval: 5.28963\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.82007\tvalid_1's custom_asymmetric_eval: 5.27632\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.76578\tvalid_1's custom_asymmetric_eval: 5.26871\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.71463\tvalid_1's custom_asymmetric_eval: 5.26377\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.66717\tvalid_1's custom_asymmetric_eval: 5.26101\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.62257\tvalid_1's custom_asymmetric_eval: 5.25603\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.57995\tvalid_1's custom_asymmetric_eval: 5.25585\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.54123\tvalid_1's custom_asymmetric_eval: 5.25273\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.50379\tvalid_1's custom_asymmetric_eval: 5.24827\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.46812\tvalid_1's custom_asymmetric_eval: 5.24196\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.43457\tvalid_1's custom_asymmetric_eval: 5.23659\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.4015\tvalid_1's custom_asymmetric_eval: 5.23192\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.37056\tvalid_1's custom_asymmetric_eval: 5.22733\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.3417\tvalid_1's custom_asymmetric_eval: 5.22487\n",
      "[1150]\ttraining's custom_asymmetric_eval: 3.31369\tvalid_1's custom_asymmetric_eval: 5.2238\n",
      "[1200]\ttraining's custom_asymmetric_eval: 3.28677\tvalid_1's custom_asymmetric_eval: 5.2215\n",
      "[1250]\ttraining's custom_asymmetric_eval: 3.26179\tvalid_1's custom_asymmetric_eval: 5.22057\n",
      "[1300]\ttraining's custom_asymmetric_eval: 3.23717\tvalid_1's custom_asymmetric_eval: 5.21787\n",
      "[1350]\ttraining's custom_asymmetric_eval: 3.21261\tvalid_1's custom_asymmetric_eval: 5.21832\n",
      "[1400]\ttraining's custom_asymmetric_eval: 3.18985\tvalid_1's custom_asymmetric_eval: 5.21827\n",
      "Early stopping, best iteration is:\n",
      "[1375]\ttraining's custom_asymmetric_eval: 3.20097\tvalid_1's custom_asymmetric_eval: 5.21714\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.443221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6590\n",
      "[LightGBM] [Info] Number of data points in the train set: 21769860, number of used features: 38\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 5.0322\tvalid_1's custom_asymmetric_eval: 6.00391\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.6434\tvalid_1's custom_asymmetric_eval: 5.80338\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.42428\tvalid_1's custom_asymmetric_eval: 5.70393\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.27875\tvalid_1's custom_asymmetric_eval: 5.64943\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.16245\tvalid_1's custom_asymmetric_eval: 5.61004\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.07247\tvalid_1's custom_asymmetric_eval: 5.58731\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.99249\tvalid_1's custom_asymmetric_eval: 5.57097\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.92034\tvalid_1's custom_asymmetric_eval: 5.55381\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.85469\tvalid_1's custom_asymmetric_eval: 5.53792\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.79543\tvalid_1's custom_asymmetric_eval: 5.5227\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.74075\tvalid_1's custom_asymmetric_eval: 5.51441\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.69329\tvalid_1's custom_asymmetric_eval: 5.50652\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.64696\tvalid_1's custom_asymmetric_eval: 5.49792\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.60185\tvalid_1's custom_asymmetric_eval: 5.49136\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.55882\tvalid_1's custom_asymmetric_eval: 5.48242\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.52091\tvalid_1's custom_asymmetric_eval: 5.47486\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.48447\tvalid_1's custom_asymmetric_eval: 5.46861\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.45085\tvalid_1's custom_asymmetric_eval: 5.4629\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.41689\tvalid_1's custom_asymmetric_eval: 5.4547\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.38571\tvalid_1's custom_asymmetric_eval: 5.45085\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.3556\tvalid_1's custom_asymmetric_eval: 5.4467\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.32697\tvalid_1's custom_asymmetric_eval: 5.44649\n",
      "[1150]\ttraining's custom_asymmetric_eval: 3.29922\tvalid_1's custom_asymmetric_eval: 5.44288\n",
      "[1200]\ttraining's custom_asymmetric_eval: 3.27291\tvalid_1's custom_asymmetric_eval: 5.44275\n",
      "[1250]\ttraining's custom_asymmetric_eval: 3.24865\tvalid_1's custom_asymmetric_eval: 5.43813\n",
      "[1300]\ttraining's custom_asymmetric_eval: 3.2238\tvalid_1's custom_asymmetric_eval: 5.4346\n",
      "[1350]\ttraining's custom_asymmetric_eval: 3.19874\tvalid_1's custom_asymmetric_eval: 5.4303\n",
      "[1400]\ttraining's custom_asymmetric_eval: 3.17628\tvalid_1's custom_asymmetric_eval: 5.4273\n",
      "[1450]\ttraining's custom_asymmetric_eval: 3.15492\tvalid_1's custom_asymmetric_eval: 5.42426\n",
      "[1500]\ttraining's custom_asymmetric_eval: 3.13347\tvalid_1's custom_asymmetric_eval: 5.42112\n",
      "[1550]\ttraining's custom_asymmetric_eval: 3.11284\tvalid_1's custom_asymmetric_eval: 5.4184\n",
      "[1600]\ttraining's custom_asymmetric_eval: 3.09295\tvalid_1's custom_asymmetric_eval: 5.41752\n",
      "[1650]\ttraining's custom_asymmetric_eval: 3.07296\tvalid_1's custom_asymmetric_eval: 5.41495\n",
      "[1700]\ttraining's custom_asymmetric_eval: 3.05385\tvalid_1's custom_asymmetric_eval: 5.41279\n",
      "[1750]\ttraining's custom_asymmetric_eval: 3.03582\tvalid_1's custom_asymmetric_eval: 5.41291\n",
      "[1800]\ttraining's custom_asymmetric_eval: 3.01677\tvalid_1's custom_asymmetric_eval: 5.40985\n",
      "[1850]\ttraining's custom_asymmetric_eval: 2.99932\tvalid_1's custom_asymmetric_eval: 5.40784\n",
      "[1900]\ttraining's custom_asymmetric_eval: 2.98122\tvalid_1's custom_asymmetric_eval: 5.40448\n",
      "[1950]\ttraining's custom_asymmetric_eval: 2.96446\tvalid_1's custom_asymmetric_eval: 5.40371\n",
      "[2000]\ttraining's custom_asymmetric_eval: 2.94796\tvalid_1's custom_asymmetric_eval: 5.40274\n",
      "[2050]\ttraining's custom_asymmetric_eval: 2.93159\tvalid_1's custom_asymmetric_eval: 5.39903\n",
      "[2100]\ttraining's custom_asymmetric_eval: 2.9151\tvalid_1's custom_asymmetric_eval: 5.39864\n",
      "[2150]\ttraining's custom_asymmetric_eval: 2.90023\tvalid_1's custom_asymmetric_eval: 5.39817\n",
      "[2200]\ttraining's custom_asymmetric_eval: 2.88496\tvalid_1's custom_asymmetric_eval: 5.39717\n",
      "[2250]\ttraining's custom_asymmetric_eval: 2.86935\tvalid_1's custom_asymmetric_eval: 5.39576\n",
      "[2300]\ttraining's custom_asymmetric_eval: 2.85515\tvalid_1's custom_asymmetric_eval: 5.39506\n",
      "[2350]\ttraining's custom_asymmetric_eval: 2.84148\tvalid_1's custom_asymmetric_eval: 5.39524\n",
      "Early stopping, best iteration is:\n",
      "[2330]\ttraining's custom_asymmetric_eval: 2.84708\tvalid_1's custom_asymmetric_eval: 5.39467\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.380064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6580\n",
      "[LightGBM] [Info] Number of data points in the train set: 21769860, number of used features: 38\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 5.17088\tvalid_1's custom_asymmetric_eval: 5.39618\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.75967\tvalid_1's custom_asymmetric_eval: 5.20527\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.54446\tvalid_1's custom_asymmetric_eval: 5.13559\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.39244\tvalid_1's custom_asymmetric_eval: 5.08962\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.27418\tvalid_1's custom_asymmetric_eval: 5.05847\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.17302\tvalid_1's custom_asymmetric_eval: 5.04184\n",
      "[350]\ttraining's custom_asymmetric_eval: 4.08988\tvalid_1's custom_asymmetric_eval: 5.02463\n",
      "[400]\ttraining's custom_asymmetric_eval: 4.01076\tvalid_1's custom_asymmetric_eval: 5.00931\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.93794\tvalid_1's custom_asymmetric_eval: 4.99079\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.87565\tvalid_1's custom_asymmetric_eval: 4.98132\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.81952\tvalid_1's custom_asymmetric_eval: 4.97118\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.76716\tvalid_1's custom_asymmetric_eval: 4.96523\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.71933\tvalid_1's custom_asymmetric_eval: 4.96093\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.67319\tvalid_1's custom_asymmetric_eval: 4.95325\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.62728\tvalid_1's custom_asymmetric_eval: 4.94786\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.58768\tvalid_1's custom_asymmetric_eval: 4.94615\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.54788\tvalid_1's custom_asymmetric_eval: 4.94015\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.51134\tvalid_1's custom_asymmetric_eval: 4.93841\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.4777\tvalid_1's custom_asymmetric_eval: 4.93375\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.4454\tvalid_1's custom_asymmetric_eval: 4.93058\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.41361\tvalid_1's custom_asymmetric_eval: 4.92256\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.38339\tvalid_1's custom_asymmetric_eval: 4.92052\n",
      "[1150]\ttraining's custom_asymmetric_eval: 3.35457\tvalid_1's custom_asymmetric_eval: 4.91817\n",
      "[1200]\ttraining's custom_asymmetric_eval: 3.32758\tvalid_1's custom_asymmetric_eval: 4.91476\n",
      "[1250]\ttraining's custom_asymmetric_eval: 3.3021\tvalid_1's custom_asymmetric_eval: 4.91346\n",
      "[1300]\ttraining's custom_asymmetric_eval: 3.27546\tvalid_1's custom_asymmetric_eval: 4.90977\n",
      "[1350]\ttraining's custom_asymmetric_eval: 3.25126\tvalid_1's custom_asymmetric_eval: 4.9071\n",
      "[1400]\ttraining's custom_asymmetric_eval: 3.22685\tvalid_1's custom_asymmetric_eval: 4.90483\n",
      "[1450]\ttraining's custom_asymmetric_eval: 3.20421\tvalid_1's custom_asymmetric_eval: 4.90404\n",
      "[1500]\ttraining's custom_asymmetric_eval: 3.18143\tvalid_1's custom_asymmetric_eval: 4.90258\n",
      "[1550]\ttraining's custom_asymmetric_eval: 3.16123\tvalid_1's custom_asymmetric_eval: 4.90108\n",
      "[1600]\ttraining's custom_asymmetric_eval: 3.14134\tvalid_1's custom_asymmetric_eval: 4.89952\n",
      "[1650]\ttraining's custom_asymmetric_eval: 3.12065\tvalid_1's custom_asymmetric_eval: 4.89892\n",
      "[1700]\ttraining's custom_asymmetric_eval: 3.10061\tvalid_1's custom_asymmetric_eval: 4.8971\n",
      "[1750]\ttraining's custom_asymmetric_eval: 3.08153\tvalid_1's custom_asymmetric_eval: 4.89681\n",
      "[1800]\ttraining's custom_asymmetric_eval: 3.06219\tvalid_1's custom_asymmetric_eval: 4.896\n",
      "[1850]\ttraining's custom_asymmetric_eval: 3.04275\tvalid_1's custom_asymmetric_eval: 4.89462\n",
      "[1900]\ttraining's custom_asymmetric_eval: 3.02464\tvalid_1's custom_asymmetric_eval: 4.89312\n",
      "[1950]\ttraining's custom_asymmetric_eval: 3.00679\tvalid_1's custom_asymmetric_eval: 4.89275\n",
      "[2000]\ttraining's custom_asymmetric_eval: 2.9896\tvalid_1's custom_asymmetric_eval: 4.89233\n",
      "[2050]\ttraining's custom_asymmetric_eval: 2.97287\tvalid_1's custom_asymmetric_eval: 4.89238\n",
      "Early stopping, best iteration is:\n",
      "[2026]\ttraining's custom_asymmetric_eval: 2.98076\tvalid_1's custom_asymmetric_eval: 4.89133\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 4\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.607198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6587\n",
      "[LightGBM] [Info] Number of data points in the train set: 21769860, number of used features: 38\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 5.06589\tvalid_1's custom_asymmetric_eval: 5.71545\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.6778\tvalid_1's custom_asymmetric_eval: 5.51295\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.46868\tvalid_1's custom_asymmetric_eval: 5.44004\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.32529\tvalid_1's custom_asymmetric_eval: 5.38796\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.21401\tvalid_1's custom_asymmetric_eval: 5.35872\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.1143\tvalid_1's custom_asymmetric_eval: 5.33035\n",
      "[350]\ttraining's custom_asymmetric_eval: 4.02896\tvalid_1's custom_asymmetric_eval: 5.30882\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.9533\tvalid_1's custom_asymmetric_eval: 5.29121\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.88658\tvalid_1's custom_asymmetric_eval: 5.2771\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.82497\tvalid_1's custom_asymmetric_eval: 5.26781\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.76981\tvalid_1's custom_asymmetric_eval: 5.25426\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.71887\tvalid_1's custom_asymmetric_eval: 5.24486\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.67131\tvalid_1's custom_asymmetric_eval: 5.23401\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.62777\tvalid_1's custom_asymmetric_eval: 5.22593\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.58379\tvalid_1's custom_asymmetric_eval: 5.22104\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.54412\tvalid_1's custom_asymmetric_eval: 5.21447\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.50648\tvalid_1's custom_asymmetric_eval: 5.20642\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.46994\tvalid_1's custom_asymmetric_eval: 5.19909\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.43569\tvalid_1's custom_asymmetric_eval: 5.19513\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.40112\tvalid_1's custom_asymmetric_eval: 5.18812\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.36862\tvalid_1's custom_asymmetric_eval: 5.18354\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.33929\tvalid_1's custom_asymmetric_eval: 5.18062\n",
      "[1150]\ttraining's custom_asymmetric_eval: 3.31168\tvalid_1's custom_asymmetric_eval: 5.17698\n",
      "[1200]\ttraining's custom_asymmetric_eval: 3.28395\tvalid_1's custom_asymmetric_eval: 5.17226\n",
      "[1250]\ttraining's custom_asymmetric_eval: 3.2573\tvalid_1's custom_asymmetric_eval: 5.16951\n",
      "[1300]\ttraining's custom_asymmetric_eval: 3.23273\tvalid_1's custom_asymmetric_eval: 5.16598\n",
      "[1350]\ttraining's custom_asymmetric_eval: 3.21009\tvalid_1's custom_asymmetric_eval: 5.16345\n",
      "[1400]\ttraining's custom_asymmetric_eval: 3.18625\tvalid_1's custom_asymmetric_eval: 5.16041\n",
      "[1450]\ttraining's custom_asymmetric_eval: 3.16494\tvalid_1's custom_asymmetric_eval: 5.15834\n",
      "[1500]\ttraining's custom_asymmetric_eval: 3.14305\tvalid_1's custom_asymmetric_eval: 5.15618\n",
      "[1550]\ttraining's custom_asymmetric_eval: 3.12127\tvalid_1's custom_asymmetric_eval: 5.15322\n",
      "[1600]\ttraining's custom_asymmetric_eval: 3.10021\tvalid_1's custom_asymmetric_eval: 5.15247\n",
      "[1650]\ttraining's custom_asymmetric_eval: 3.08011\tvalid_1's custom_asymmetric_eval: 5.14994\n",
      "[1700]\ttraining's custom_asymmetric_eval: 3.06005\tvalid_1's custom_asymmetric_eval: 5.14802\n",
      "[1750]\ttraining's custom_asymmetric_eval: 3.0411\tvalid_1's custom_asymmetric_eval: 5.14759\n",
      "[1800]\ttraining's custom_asymmetric_eval: 3.02245\tvalid_1's custom_asymmetric_eval: 5.14572\n",
      "[1850]\ttraining's custom_asymmetric_eval: 3.00456\tvalid_1's custom_asymmetric_eval: 5.14535\n",
      "[1900]\ttraining's custom_asymmetric_eval: 2.98608\tvalid_1's custom_asymmetric_eval: 5.14339\n",
      "[1950]\ttraining's custom_asymmetric_eval: 2.96923\tvalid_1's custom_asymmetric_eval: 5.14207\n",
      "[2000]\ttraining's custom_asymmetric_eval: 2.95308\tvalid_1's custom_asymmetric_eval: 5.1408\n",
      "[2050]\ttraining's custom_asymmetric_eval: 2.93695\tvalid_1's custom_asymmetric_eval: 5.14092\n",
      "Early stopping, best iteration is:\n",
      "[2034]\ttraining's custom_asymmetric_eval: 2.94186\tvalid_1's custom_asymmetric_eval: 5.14064\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.458526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6593\n",
      "[LightGBM] [Info] Number of data points in the train set: 21800350, number of used features: 38\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 5.08575\tvalid_1's custom_asymmetric_eval: 5.52617\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.69037\tvalid_1's custom_asymmetric_eval: 5.32425\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.48862\tvalid_1's custom_asymmetric_eval: 5.23258\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.34256\tvalid_1's custom_asymmetric_eval: 5.18345\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.21742\tvalid_1's custom_asymmetric_eval: 5.13644\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.12175\tvalid_1's custom_asymmetric_eval: 5.10849\n",
      "[350]\ttraining's custom_asymmetric_eval: 4.03877\tvalid_1's custom_asymmetric_eval: 5.08866\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.96361\tvalid_1's custom_asymmetric_eval: 5.07415\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.89527\tvalid_1's custom_asymmetric_eval: 5.06024\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.83475\tvalid_1's custom_asymmetric_eval: 5.04931\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.77876\tvalid_1's custom_asymmetric_eval: 5.03596\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.72675\tvalid_1's custom_asymmetric_eval: 5.02792\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.67806\tvalid_1's custom_asymmetric_eval: 5.01951\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.63193\tvalid_1's custom_asymmetric_eval: 5.00938\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.58887\tvalid_1's custom_asymmetric_eval: 4.99949\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.54918\tvalid_1's custom_asymmetric_eval: 4.99247\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.50937\tvalid_1's custom_asymmetric_eval: 4.98456\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.47509\tvalid_1's custom_asymmetric_eval: 4.98101\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.44056\tvalid_1's custom_asymmetric_eval: 4.9757\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.40878\tvalid_1's custom_asymmetric_eval: 4.97309\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.3791\tvalid_1's custom_asymmetric_eval: 4.96851\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.34875\tvalid_1's custom_asymmetric_eval: 4.96435\n",
      "[1150]\ttraining's custom_asymmetric_eval: 3.32106\tvalid_1's custom_asymmetric_eval: 4.95967\n",
      "[1200]\ttraining's custom_asymmetric_eval: 3.2929\tvalid_1's custom_asymmetric_eval: 4.95452\n",
      "[1250]\ttraining's custom_asymmetric_eval: 3.26698\tvalid_1's custom_asymmetric_eval: 4.9503\n",
      "[1300]\ttraining's custom_asymmetric_eval: 3.24193\tvalid_1's custom_asymmetric_eval: 4.94719\n",
      "[1350]\ttraining's custom_asymmetric_eval: 3.21757\tvalid_1's custom_asymmetric_eval: 4.94376\n",
      "[1400]\ttraining's custom_asymmetric_eval: 3.19364\tvalid_1's custom_asymmetric_eval: 4.94136\n",
      "[1450]\ttraining's custom_asymmetric_eval: 3.171\tvalid_1's custom_asymmetric_eval: 4.93795\n",
      "[1500]\ttraining's custom_asymmetric_eval: 3.1489\tvalid_1's custom_asymmetric_eval: 4.93613\n",
      "[1550]\ttraining's custom_asymmetric_eval: 3.12747\tvalid_1's custom_asymmetric_eval: 4.93332\n",
      "[1600]\ttraining's custom_asymmetric_eval: 3.10686\tvalid_1's custom_asymmetric_eval: 4.93041\n",
      "[1650]\ttraining's custom_asymmetric_eval: 3.08758\tvalid_1's custom_asymmetric_eval: 4.92867\n",
      "[1700]\ttraining's custom_asymmetric_eval: 3.06758\tvalid_1's custom_asymmetric_eval: 4.92578\n",
      "[1750]\ttraining's custom_asymmetric_eval: 3.04836\tvalid_1's custom_asymmetric_eval: 4.92437\n",
      "[1800]\ttraining's custom_asymmetric_eval: 3.02968\tvalid_1's custom_asymmetric_eval: 4.92215\n",
      "[1850]\ttraining's custom_asymmetric_eval: 3.01235\tvalid_1's custom_asymmetric_eval: 4.92067\n",
      "[1900]\ttraining's custom_asymmetric_eval: 2.99431\tvalid_1's custom_asymmetric_eval: 4.91776\n",
      "[1950]\ttraining's custom_asymmetric_eval: 2.97718\tvalid_1's custom_asymmetric_eval: 4.91607\n",
      "[2000]\ttraining's custom_asymmetric_eval: 2.96087\tvalid_1's custom_asymmetric_eval: 4.91489\n",
      "[2050]\ttraining's custom_asymmetric_eval: 2.94419\tvalid_1's custom_asymmetric_eval: 4.91379\n",
      "[2100]\ttraining's custom_asymmetric_eval: 2.92817\tvalid_1's custom_asymmetric_eval: 4.91169\n",
      "[2150]\ttraining's custom_asymmetric_eval: 2.91213\tvalid_1's custom_asymmetric_eval: 4.91138\n",
      "[2200]\ttraining's custom_asymmetric_eval: 2.8971\tvalid_1's custom_asymmetric_eval: 4.91047\n",
      "[2250]\ttraining's custom_asymmetric_eval: 2.88281\tvalid_1's custom_asymmetric_eval: 4.91047\n",
      "[2300]\ttraining's custom_asymmetric_eval: 2.86819\tvalid_1's custom_asymmetric_eval: 4.90852\n",
      "[2350]\ttraining's custom_asymmetric_eval: 2.85287\tvalid_1's custom_asymmetric_eval: 4.9073\n",
      "[2400]\ttraining's custom_asymmetric_eval: 2.8391\tvalid_1's custom_asymmetric_eval: 4.9071\n",
      "[2450]\ttraining's custom_asymmetric_eval: 2.82518\tvalid_1's custom_asymmetric_eval: 4.90595\n",
      "[2500]\ttraining's custom_asymmetric_eval: 2.8114\tvalid_1's custom_asymmetric_eval: 4.90605\n",
      "[2550]\ttraining's custom_asymmetric_eval: 2.79811\tvalid_1's custom_asymmetric_eval: 4.90498\n",
      "[2600]\ttraining's custom_asymmetric_eval: 2.78483\tvalid_1's custom_asymmetric_eval: 4.90379\n",
      "[2650]\ttraining's custom_asymmetric_eval: 2.77248\tvalid_1's custom_asymmetric_eval: 4.90278\n",
      "[2700]\ttraining's custom_asymmetric_eval: 2.75987\tvalid_1's custom_asymmetric_eval: 4.90167\n",
      "[2750]\ttraining's custom_asymmetric_eval: 2.74765\tvalid_1's custom_asymmetric_eval: 4.90118\n",
      "[2800]\ttraining's custom_asymmetric_eval: 2.73538\tvalid_1's custom_asymmetric_eval: 4.90017\n",
      "[2850]\ttraining's custom_asymmetric_eval: 2.72339\tvalid_1's custom_asymmetric_eval: 4.89892\n",
      "[2900]\ttraining's custom_asymmetric_eval: 2.7114\tvalid_1's custom_asymmetric_eval: 4.89811\n",
      "[2950]\ttraining's custom_asymmetric_eval: 2.69938\tvalid_1's custom_asymmetric_eval: 4.89777\n",
      "Early stopping, best iteration is:\n",
      "[2927]\ttraining's custom_asymmetric_eval: 2.70464\tvalid_1's custom_asymmetric_eval: 4.89742\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our out of folds rmse is 2.157380645939324\n"
     ]
    }
   ],
   "source": [
    "# define our numeric features and categorical features\n",
    "features = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \"event_name_1\", \"event_type_1\", \n",
    "            \"event_name_2\", \"event_type_2\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"sell_price\",\n",
    "    # demand features.\n",
    "    \"demand_rolling_mean_t7\", \"demand_rolling_mean_t15\", \"demand_rolling_mean_t30\", \"demand_rolling_mean_t90\",\n",
    "    \"demand_rolling_std_t7\", \"demand_rolling_std_t15\", \"demand_rolling_std_t30\", \"demand_rolling_std_t90\",\n",
    "            \n",
    "    # price features\n",
    "    \"price_change_t1\", \"price_change_t365\", \"rolling_price_std_t7\", \"rolling_price_std_t30\",\n",
    "    # time features.\n",
    "    \"year\", \"month\", \"week\", \"day\", \"dayofweek\", \"dayofyear\", \"is_year_end\", \"is_year_start\", \"is_quarter_end\", \"is_quarter_start\",\n",
    "    \"is_month_end\", \"is_month_start\", \"is_weekend\",\n",
    "]\n",
    "\n",
    "# features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'year', \n",
    "#                 'month', 'quarter', 'week', 'day', 'dayofweek', 'dayofyear', 'demand_rolling_mean_t7', 'demand_rolling_mean_t15', 'demand_rolling_mean_t30', 'demand_rolling_mean_t90',\n",
    "#                 'demand_rolling_std_t7', 'demand_rolling_std_t15', 'demand_rolling_std_t30', 'demand_rolling_std_t90']\n",
    "\n",
    "cat_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', \n",
    "                'event_name_2', 'event_type_2']\n",
    "\n",
    "print('-'*50)\n",
    "print('\\n')\n",
    "print(f'Training model with {len(features)} features...')\n",
    "# run lgbm model with 5 GroupKFold (subgroups by year, month)\n",
    "test = run_lgb(data, features, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27227570</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>0.927728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27227571</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>0.314889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27227572</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>0.428381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27227573</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>2.163015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27227574</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>0.986496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id       date    demand\n",
       "27227570  HOBBIES_1_001_CA_1_validation 2016-04-25  0.927728\n",
       "27227571  HOBBIES_1_002_CA_1_validation 2016-04-25  0.314889\n",
       "27227572  HOBBIES_1_003_CA_1_validation 2016-04-25  0.428381\n",
       "27227573  HOBBIES_1_004_CA_1_validation 2016-04-25  2.163015\n",
       "27227574  HOBBIES_1_005_CA_1_validation 2016-04-25  0.986496"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save predictions...\n",
      "Prediction is generated...\n"
     ]
    }
   ],
   "source": [
    "print('Save predictions...')\n",
    "# predict\n",
    "predict(test, submission)\n",
    "print(\"Prediction is generated...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper functions to reduce memory\n",
    "# def reduce_mem_usage(df, verbose=True):\n",
    "#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "#     start_mem = df.memory_usage().sum() / 1024**2    \n",
    "#     for col in df.columns:\n",
    "#         col_type = df[col].dtypes\n",
    "#         if col_type in numerics:\n",
    "#             c_min = df[col].min()\n",
    "#             c_max = df[col].max()\n",
    "#             if str(col_type)[:3] == 'int':\n",
    "#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "#                     df[col] = df[col].astype(np.int8)\n",
    "#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "#                     df[col] = df[col].astype(np.int16)\n",
    "#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "#                     df[col] = df[col].astype(np.int32)\n",
    "#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "#                     df[col] = df[col].astype(np.int64)  \n",
    "#             else:\n",
    "#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "#                     df[col] = df[col].astype(np.float16)\n",
    "#                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "#                     df[col] = df[col].astype(np.float32)\n",
    "#                 else:\n",
    "#                     df[col] = df[col].astype(np.float64)    \n",
    "#     end_mem = df.memory_usage().sum() / 1024**2\n",
    "#     if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "#     return df\n",
    "\n",
    "# # function to read our data\n",
    "# def read_data():\n",
    "#     # read data\n",
    "#     data = pd.read_pickle('/kaggle/input/m5-reduce-data/data_small.pkl')\n",
    "#     # fillna and label encode categorical features\n",
    "#     data = transform(data)\n",
    "#     # read submission\n",
    "#     submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n",
    "#     return data, submission\n",
    "\n",
    "# # filla na and label encode categorical features\n",
    "# def transform(data):\n",
    "    \n",
    "#     nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "#     for feature in nan_features:\n",
    "#         data[feature].fillna('unknown', inplace = True)\n",
    "        \n",
    "#     cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', \n",
    "#            'event_name_2', 'event_type_2']\n",
    "#     for feature in cat:\n",
    "#         encoder = preprocessing.LabelEncoder()\n",
    "#         data[feature] = encoder.fit_transform(data[feature])\n",
    "        \n",
    "#     # reduce memory usage\n",
    "#     data = reduce_mem_usage(data)\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# # simple feature ingineer function\n",
    "# def simple_fe(data):\n",
    "    \n",
    "#     data_fe = data[['id', 'demand']]\n",
    "    \n",
    "#     window = 28\n",
    "#     periods = [7, 15, 30, 90]\n",
    "#     group = data_fe.groupby('id')['demand']\n",
    "    \n",
    "#     # most recent lag data\n",
    "#     for period in periods:\n",
    "#         data_fe['demand_rolling_mean_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).mean())\n",
    "#         data_fe['demand_rolling_std_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).std())\n",
    "        \n",
    "#     # reduce memory\n",
    "#     data_fe = reduce_mem_usage(data_fe)\n",
    "    \n",
    "#     # get time features\n",
    "#     data['date'] = pd.to_datetime(data['date'])\n",
    "#     time_features = ['year', 'month', 'quarter', 'week', 'day', 'dayofweek', 'dayofyear']\n",
    "#     dtype = np.int16\n",
    "#     for time_feature in time_features:\n",
    "#         data[time_feature] = getattr(data['date'].dt, time_feature).astype(dtype)\n",
    "        \n",
    "#     # concat lag and rolling features with main table\n",
    "#     lag_rolling_features = [col for col in data_fe.columns if col not in ['id', 'demand']]\n",
    "#     data = pd.concat([data, data_fe[lag_rolling_features]], axis = 1)\n",
    "    \n",
    "#     del data_fe\n",
    "#     gc.collect()\n",
    "\n",
    "#     return data\n",
    "\n",
    "# # define custom loss function\n",
    "# def custom_asymmetric_train(y_pred, y_true):\n",
    "#     y_true = y_true.get_label()\n",
    "#     residual = (y_true - y_pred).astype(\"float\")\n",
    "#     grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n",
    "#     hess = np.where(residual < 0, 2, 2 * 1.15)\n",
    "#     return grad, hess\n",
    "\n",
    "# # define custom evaluation metric\n",
    "# def custom_asymmetric_valid(y_pred, y_true):\n",
    "#     y_true = y_true.get_label()\n",
    "#     residual = (y_true - y_pred).astype(\"float\")\n",
    "#     loss = np.where(residual < 0, (residual ** 2) , (residual ** 2) * 1.15) \n",
    "#     return \"custom_asymmetric_eval\", np.mean(loss), False\n",
    "\n",
    "# # define lgbm simple model\n",
    "# def run_lgb(data, features, cat_features):\n",
    "    \n",
    "#     # reset_index\n",
    "#     data.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "#     # going to evaluate with the last 28 days\n",
    "#     x_train = data[data['date'] <= '2016-04-24']\n",
    "#     y_train = x_train['demand']\n",
    "#     test = data[data['date'] >= '2016-04-25']\n",
    "\n",
    "#     # define random hyperparammeters\n",
    "#     params = {\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'n_jobs': -1,\n",
    "#         'seed': 42,\n",
    "#         'learning_rate': 0.1,\n",
    "#         'bagging_fraction': 0.85,\n",
    "#         'bagging_freq': 1, \n",
    "#         'colsample_bytree': 0.85,\n",
    "#         'colsample_bynode': 0.85,\n",
    "#         'min_data_per_leaf': 25,\n",
    "#         'num_leaves': 200,\n",
    "#         'lambda_l1': 0.5,\n",
    "#         'lambda_l2': 0.5}\n",
    "    \n",
    "#     oof = np.zeros(len(x_train))\n",
    "#     preds = np.zeros(len(test))\n",
    "    \n",
    "#     # GroupKFold by week, month to avoid leakage and overfitting (not entirely sure xD)\n",
    "#     kf = GroupKFold(5)\n",
    "#     # get subgroups for each week, year pair\n",
    "#     group = x_train['week'].astype(str) + '_' + x_train['year'].astype(str)\n",
    "#     for fold, (trn_idx, val_idx) in enumerate(kf.split(x_train, y_train, group)):\n",
    "#         print(f'Training fold {fold + 1}')\n",
    "#         train_set = lgb.Dataset(x_train.iloc[trn_idx][features], y_train.iloc[trn_idx], \n",
    "#                                 categorical_feature = cat_features)\n",
    "#         val_set = lgb.Dataset(x_train.iloc[val_idx][features], y_train.iloc[val_idx], \n",
    "#                               categorical_feature = cat_features)\n",
    "        \n",
    "#         # train with our custom loss function and evaluation metric\n",
    "#         model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 50, \n",
    "#                           valid_sets = [train_set, val_set], verbose_eval = 50, fobj = custom_asymmetric_train, \n",
    "#                           feval = custom_asymmetric_valid)\n",
    "    \n",
    "#         # predict oof\n",
    "#         oof[val_idx] = model.predict(x_train.iloc[val_idx][features])\n",
    "\n",
    "#         # predict test\n",
    "#         preds += model.predict(test[features]) / 5\n",
    "        \n",
    "#         print('-'*50)\n",
    "#         print('\\n')\n",
    "        \n",
    "#     oof_rmse = np.sqrt(metrics.mean_squared_error(y_train, oof))\n",
    "#     print(f'Our out of folds rmse is {oof_rmse}')\n",
    "        \n",
    "#     test = test[['id', 'date', 'demand']]\n",
    "#     test['demand'] = preds\n",
    "#     return test\n",
    "\n",
    "# # function to get the predictions in the correct format\n",
    "# def predict(test, submission):\n",
    "#     predictions = pd.pivot(test, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "#     predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "#     evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "#     evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "#     validation = submission[['id']].merge(predictions, on = 'id')\n",
    "#     final = pd.concat([validation, evaluation])\n",
    "#     final.to_csv('submission_custom_loss.csv', index = False)\n",
    "    \n",
    "# # this is the main function that will run our entire program\n",
    "# def train_and_evaluate():\n",
    "    \n",
    "#     # read data\n",
    "#     print('Reading our data...')\n",
    "#     data, submission = read_data()\n",
    "    \n",
    "#     data['date'] = pd.to_datetime(data['date'])\n",
    "#     # get amount of unique days in our data\n",
    "#     days = abs((data['date'].min() - data['date'].max()).days)\n",
    "#     # how many training data do we need to train with at least 2 years and consider lags\n",
    "#     need = 365 + 365 + 90 + 28\n",
    "#     print(f'We have {(days - 28)} days of training history')\n",
    "#     print(f'we have {(days - 28 - need)} days left')\n",
    "#     if (days - 28 - need) > 0:\n",
    "#         print('We have enought training data, lets continue')\n",
    "#     else:\n",
    "#         print('Get more training data, training can fail')\n",
    "    \n",
    "#     # simple feature engineer\n",
    "#     print('Running simple feature engineering...')\n",
    "#     data = simple_fe(data)\n",
    "#     print('Removing first 118 days')\n",
    "#     # eliminate the first 118 days of our train data because of lags\n",
    "#     min_date = data['date'].min() + timedelta(days = 118)\n",
    "#     data = data[data['date'] > min_date]\n",
    "    \n",
    "#     # define our numeric features and categorical features\n",
    "#     features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'year', \n",
    "#                 'month', 'quarter', 'week', 'day', 'dayofweek', 'dayofyear', 'demand_rolling_mean_t7', 'demand_rolling_mean_t15', 'demand_rolling_mean_t30', 'demand_rolling_mean_t90',\n",
    "#                 'demand_rolling_std_t7', 'demand_rolling_std_t15', 'demand_rolling_std_t30', 'demand_rolling_std_t90']\n",
    "    \n",
    "#     cat_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', \n",
    "#                     'event_name_2', 'event_type_2']\n",
    "    \n",
    "#     print('-'*50)\n",
    "#     print('\\n')\n",
    "#     print(f'Training model with {len(features)} features...')\n",
    "#     # run lgbm model with 5 GroupKFold (subgroups by year, month)\n",
    "#     test = run_lgb(data, features, cat_features)\n",
    "#     print('Save predictions...')\n",
    "#     # predict\n",
    "#     predict(test, submission)\n",
    "        \n",
    "# # run our program\n",
    "# train_and_evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
